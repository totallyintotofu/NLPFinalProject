{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdvancedSearchScraper(object):\n",
    "\n",
    "    \"\"\"AdvancedSearchScraper query limit = 100\n",
    "    Class scrapes tweets from the Twitter Advanced Search results\n",
    "    Parameters:\n",
    "    - query\n",
    "      If you want to perform the search\n",
    "      https://twitter.com/search?q=python%20lang%3Aen%20since%3A2016-08-01%20&src=typd,\n",
    "      then you should use:\n",
    "      ass(LOLOLOL) = AdvancedSearchScraper(\"python%20lang%3Aen%20since%3A2016-08-01%20\")\n",
    "      Class will automatically take care of all other url parameters\n",
    "      like \"src\" etc.\n",
    "    - limit\n",
    "      limit indicates the approximate number of tweets that should be scraped.\n",
    "      The default value is 100. Set it to None to scrape all tweets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, query, limit = 100, verbose = False):\n",
    "\n",
    "        # We could have used separate parameters like language, since, until\n",
    "        # Corresponding to each form element in the Advanced Search page.\n",
    "        # But this complicates the logic of the class.\n",
    "        # We would have to validate the parameters and this is messy.\n",
    "        # We would also have to form the query, which is messy too.\n",
    "        \n",
    "\n",
    "        self.query = query\n",
    "        self.verbose = verbose\n",
    "        if limit:\n",
    "            self.limit = limit\n",
    "        else:\n",
    "            self.limit = float(\"inf\")\n",
    "\n",
    "\n",
    "    def ajax_call_params(self, oldest_tweet_id, newest_tweet_id):\n",
    "        query_dict = {\"src\" : \"typd\",\n",
    "                      \"f\" : \"tweets\",\n",
    "                      \"include_available_features\" : 1,\n",
    "                      \"include_entities\" : 1,\n",
    "                      \"reset_error_state\" : \"false\",\n",
    "                      \"max_position\" : \"TWEET-%s-%s\" %(oldest_tweet_id, newest_tweet_id),\n",
    "                      }\n",
    "        return query_dict\n",
    "\n",
    "    def scrape(self):\n",
    "        self.tweets = []\n",
    "\n",
    "        #first-page\n",
    "\n",
    "        # if q is supplied in the params dictionary, requests replaces\n",
    "        # spaces by + . this results in an unexpected final url.\n",
    "        # this is the only way to form the correct url.\n",
    "        headers = {\"user-agent\" : \"Mozilla/5.0 (X11; Linux x86_64; rv:7.0.1)\"\n",
    "                   \" Gecko/20100101 Firefox/7.7\"}\n",
    "\n",
    "        response = requests.get(\"https://twitter.com/search?q=%s\" % self.query,\n",
    "                                params = {\"src\" : \"typd\", \"f\" : \"tweets\"},\n",
    "                                verify = False, headers = headers)\n",
    "        self.tweets+=self.get_tweets_from_html(response.text)\n",
    "\n",
    "        #ajax\n",
    "        if len(self.tweets)>0:\n",
    "\n",
    "            newest_tweet_id = self.tweets[0][\"scroll_id\"]\n",
    "            oldest_tweet_id = self.tweets[-1][\"scroll_id\"]\n",
    "\n",
    "            while len(self.tweets) <= self.limit:\n",
    "\n",
    "                # rate limiting! 1 AJAX call in 5 seconds.\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "                # if q is supplied in the params dictionary, requests replaces\n",
    "                # spaces by + . this results in an unexpected final url.\n",
    "                # this is the only way to form the correct url\n",
    "\n",
    "                response = requests.get(\n",
    "                    \"https://twitter.com/i/search/timeline?q=%s\" % self.query,\n",
    "                    params = self.ajax_call_params(oldest_tweet_id, newest_tweet_id),\n",
    "                    verify = False, headers = headers)\n",
    "                json_data = json.loads(response.text)\n",
    "                self.tweets += self.get_tweets_from_html(json_data[\"items_html\"])\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\"Scraped {0} tweets so far...\".format(len(self.tweets)))\n",
    "\n",
    "                if oldest_tweet_id == self.tweets[-1][\"scroll_id\"]:\n",
    "                    break\n",
    "\n",
    "                oldest_tweet_id = self.tweets[-1][\"scroll_id\"]\n",
    "\n",
    "        if isinstance(self.limit, int):\n",
    "            return self.tweets[:self.limit]\n",
    "\n",
    "        return self.tweets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_tweets_from_html(self, html_doc):\n",
    "        tweetlist = []\n",
    "        html_soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "        tweet_soup_list = html_soup.find_all(\"div\", {\"class\" : \"original-tweet\"})\n",
    "        for tweet_soup in tweet_soup_list:\n",
    "            try:\n",
    "                tweet_dict = { \"scroll_id\" : int(tweet_soup[\"data-retweet-id\"]) }\n",
    "            except:\n",
    "                tweet_dict = { \"scroll_id\" : int(tweet_soup[\"data-tweet-id\"]) }\n",
    "            try:\n",
    "                tweet_dict[\"tweet_id\"] = int(tweet_soup[\"data-tweet-id\"])\n",
    "                tweet_dict[\"author_name\"] = tweet_soup[\"data-name\"]\n",
    "                tweet_dict[\"author_handle\"] = tweet_soup[\"data-screen-name\"]\n",
    "                tweet_dict[\"author_id\"] = int(tweet_soup[\"data-user-id\"])\n",
    "                tweet_dict[\"author_href\"] = tweet_soup.find(\n",
    "                    \"a\",{\"class\" : \"account-group\"})[\"href\"]\n",
    "                tweet_dict[\"tweet_permalink\"] = tweet_soup[\"data-permalink-path\"]\n",
    "                tweet_dict[\"tweet_text\"] = self.prettify_tweet_text_bs_element(\n",
    "                    tweet_soup.find(\"p\", {\"class\" : \"tweet-text\"}))\n",
    "                tweet_dict[\"tweet_language\"] = tweet_soup.find(\n",
    "                    \"p\", {\"class\" : \"tweet-text\"})['lang']\n",
    "                tweet_dict[\"tweet_time\"] = tweet_soup.find(\n",
    "                    \"a\",{\"class\" : \"tweet-timestamp\"})[\"title\"]\n",
    "                tweet_dict[\"tweet_timestamp\"] = tweet_soup.find(\n",
    "                    \"span\",{\"class\" : \"_timestamp\"})[\"data-time-ms\"]\n",
    "                tweet_dict[\"retweets\"] = int(tweet_soup.find(\n",
    "                    \"span\",{\"class\" : \"ProfileTweet-action--retweet\"}).find(\n",
    "                    \"span\", {\"class\" : \"ProfileTweet-actionCount\"})['data-tweet-stat-count'])\n",
    "                tweet_dict[\"favorites\"] = int(tweet_soup.find(\n",
    "                    \"span\",{\"class\" : \"ProfileTweet-action--favorite\"}).find(\n",
    "                    \"span\", {\"class\" : \"ProfileTweet-actionCount\"})['data-tweet-stat-count'])\n",
    "            except Exception as e:\n",
    "                print(\"Error while extracting information from tweet.\")\n",
    "                print(e)\n",
    "            try:\n",
    "                tweet_dict[\"retweet_id\"] = int(tweet_soup[\"data-retweet-id\"])\n",
    "                tweet_dict[\"retweeter_handle\"] = tweet_soup[\"data-retweeter\"]\n",
    "            except:\n",
    "                pass\n",
    "            tweetlist.append(tweet_dict)\n",
    "        return tweetlist\n",
    "\n",
    "    def prettify_tweet_text_bs_element(self, tweet_text_bs_element):\n",
    "        tweet_text = ''\n",
    "        for child in tweet_text_bs_element.children:\n",
    "            if isinstance(child, NavigableString):\n",
    "                tweet_text += child + \" \"\n",
    "            elif isinstance(child, Tag):\n",
    "                try:\n",
    "                    tag_class = child['class'][0]\n",
    "                    if tag_class == \"twitter-atreply\":\n",
    "                        mention = ''.join([i.string for i in child.contents])\n",
    "                        tweet_text += mention + \" \"\n",
    "                    elif tag_class == \"twitter-hashtag\":\n",
    "                        hashtag = ''.join([i.string for i in child.contents])\n",
    "                        tweet_text += hashtag + \" \"\n",
    "                    elif tag_class == \"twitter-timeline-link\":\n",
    "                        if isinstance(child[\"href\"], str):\n",
    "                            tweet_text += child[\"href\"] + \" \"\n",
    "                except:\n",
    "                    if isinstance(child.string, str):\n",
    "                        tweet_text += child.string + \" \"\n",
    "        return \" \".join(tweet_text.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
