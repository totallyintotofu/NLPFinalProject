{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "from datetime import timedelta, date\n",
    "from multiprocessing.pool import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from twitterscraper.tweet import Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ua = UserAgent()\n",
    "HEADERS_LIST = [ua.chrome, ua.google, ua['google chrome'], ua.firefox, ua.ff]\n",
    "\n",
    "INIT_URL = \"https://twitter.com/search?f=tweets&vertical=default&q={q}\"\n",
    "RELOAD_URL = \"https://twitter.com/i/search/timeline?f=tweets&vertical=\" \\\n",
    "             \"default&include_available_features=1&include_entities=1&\" \\\n",
    "             \"reset_error_state=false&src=typd&max_position={pos}&q={q}\"\n",
    "\n",
    "\n",
    "def query_single_page(url, html_response=True, retry=3):\n",
    "    \"\"\"\n",
    "    Returns tweets from the given URL.\n",
    "    :param url: The URL to get the tweets from\n",
    "    :param html_response: False, if the HTML is embedded in a JSON\n",
    "    :param retry: Number of retries if something goes wrong.\n",
    "    :return: The list of tweets, the pos argument for getting the next page.\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': random.choice(HEADERS_LIST)}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if html_response:\n",
    "            html = response.text\n",
    "        else:\n",
    "            json_resp = response.json()\n",
    "            html = json_resp['items_html']\n",
    "\n",
    "        tweets = list(Tweet.from_html(html))\n",
    "\n",
    "        if not tweets:\n",
    "            return [], None\n",
    "\n",
    "        if not html_response:\n",
    "            return tweets, json_resp['min_position']\n",
    "\n",
    "        return tweets, \"TWEET-{}-{}\".format(tweets[-1].id, tweets[0].id)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logging.exception('HTTPError {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        logging.exception('ConnectionError {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        logging.exception('TimeOut {} while requesting \"{}\"'.format(\n",
    "            e, url))\n",
    "    if retry > 0:\n",
    "        logging.info(\"Retrying...\")\n",
    "        return query_single_page(url, html_response, retry-1)\n",
    "\n",
    "    logging.error(\"Giving up.\")\n",
    "    return [], None\n",
    "\n",
    "\n",
    "def query_tweets_once(query, limit=None, num_tweets=0):\n",
    "    \"\"\"\n",
    "    Queries twitter for all the tweets you want! It will load all pages it gets\n",
    "    from twitter. However, twitter might out of a sudden stop serving new pages,\n",
    "    in that case, use the `query_tweets` method.\n",
    "    Note that this function catches the KeyboardInterrupt so it can return\n",
    "    tweets on incomplete queries if the user decides to abort.\n",
    "    :param query: Any advanced query you want to do! Compile it at\n",
    "                  https://twitter.com/search-advanced and just copy the query!\n",
    "    :param limit: Scraping will be stopped when at least ``limit`` number of\n",
    "                  items are fetched.\n",
    "    :param num_tweets: Number of tweets fetched outside this function.\n",
    "    :return:      A list of twitterscraper.Tweet objects. You will get at least\n",
    "                  ``limit`` number of items.\n",
    "    \"\"\"\n",
    "    logging.info(\"Querying {}\".format(query))\n",
    "    query = query.replace(' ', '%20').replace(\"#\", \"%23\").replace(\":\", \"%3A\")\n",
    "    pos = None\n",
    "    tweets = []\n",
    "    try:\n",
    "        while True:\n",
    "            new_tweets, pos = query_single_page(\n",
    "                INIT_URL.format(q=query) if pos is None\n",
    "                else RELOAD_URL.format(q=query, pos=pos),\n",
    "                pos is None\n",
    "            )\n",
    "            if len(new_tweets) == 0:\n",
    "                logging.info(\"Got {} tweets for {}.\".format(\n",
    "                    len(tweets), query))\n",
    "                return tweets\n",
    "\n",
    "            logging.info(\"Got {} tweets ({} new).\".format(\n",
    "                len(tweets) + num_tweets, len(new_tweets)))\n",
    "\n",
    "            tweets += new_tweets\n",
    "\n",
    "            if limit is not None and len(tweets) + num_tweets >= limit:\n",
    "                return tweets\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Program interrupted by user. Returning tweets gathered \"\n",
    "                     \"so far...\")\n",
    "    except BaseException:\n",
    "        logging.exception(\"An unknown error occurred! Returning tweets \"\n",
    "                          \"gathered so far.\")\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def eliminate_duplicates(iterable):\n",
    "    \"\"\"\n",
    "    Yields all unique elements of an iterable sorted. Elements are considered\n",
    "    non unique if the equality comparison to another element is true. (In those\n",
    "    cases, the set conversion isn't sufficient as it uses identity comparison.)\n",
    "    \"\"\"\n",
    "    class NoElement: pass\n",
    "\n",
    "    prev_elem = NoElement\n",
    "    for elem in sorted(iterable):\n",
    "        if prev_elem is NoElement:\n",
    "            prev_elem = elem\n",
    "            yield elem\n",
    "            continue\n",
    "\n",
    "        if prev_elem != elem:\n",
    "            prev_elem = elem\n",
    "            yield elem\n",
    "\n",
    "\n",
    "def query_tweets(query, limit=None):\n",
    "    tweets = []\n",
    "    iteration = 1\n",
    "\n",
    "    while limit is None or len(tweets) < limit:\n",
    "        logging.info(\"Running iteration no {}, query is {}\".format(\n",
    "            iteration, repr(query)))\n",
    "        new_tweets = query_tweets_once(query, limit, len(tweets))\n",
    "        tweets.extend(new_tweets)\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "\n",
    "        mindate = min(map(lambda tweet: tweet.timestamp, new_tweets)).date()\n",
    "        maxdate = max(map(lambda tweet: tweet.timestamp, new_tweets)).date()\n",
    "        logging.info(\"Got tweets ranging from {} to {}\".format(\n",
    "            mindate.isoformat(), maxdate.isoformat()))\n",
    "\n",
    "        # Add a day, twitter only searches until excluding that day and we dont\n",
    "        # have complete results for that one yet. However, we cannot limit the\n",
    "        # search to less than one day: if all results are from the same day, we\n",
    "        # want to continue searching further into the past: either there are no\n",
    "        # further results or twitter stopped serving them and there's nothing\n",
    "        # we can do.\n",
    "        if mindate != maxdate:\n",
    "            mindate += timedelta(days=1)\n",
    "\n",
    "        # Twitter will always choose the more restrictive until:\n",
    "        query += ' until:' + mindate.isoformat()\n",
    "        iteration += 1\n",
    "\n",
    "    # Eliminate duplicates\n",
    "    return list(eliminate_duplicates(tweets))\n",
    "\n",
    "\n",
    "def query_all_tweets(query):\n",
    "    \"\"\"\n",
    "    Queries *all* tweets in the history of twitter for the given query. This\n",
    "    will run in parallel for each ~10 days.\n",
    "    :param query: A twitter advanced search query.\n",
    "    :return: A list of tweets.\n",
    "    \"\"\"\n",
    "    year = 2006\n",
    "    month = 3\n",
    "\n",
    "    limits = []\n",
    "    while date(year=year, month=month, day=1) < date.today():\n",
    "        nextmonth = month + 1 if month < 12 else 1\n",
    "        nextyear = year + 1 if nextmonth == 1 else year\n",
    "\n",
    "        limits.append(\n",
    "            (date(year=year, month=month, day=1),\n",
    "             date(year=year, month=month, day=10))\n",
    "        )\n",
    "        limits.append(\n",
    "            (date(year=year, month=month, day=10),\n",
    "             date(year=year, month=month, day=20))\n",
    "        )\n",
    "        limits.append(\n",
    "            (date(year=year, month=month, day=20),\n",
    "             date(year=nextyear, month=nextmonth, day=1))\n",
    "        )\n",
    "        year, month = nextyear, nextmonth\n",
    "\n",
    "    queries = ['{} since:{} until:{}'.format(query, since, until)\n",
    "               for since, until in reversed(limits)]\n",
    "\n",
    "    pool = Pool(20)\n",
    "    all_tweets = []\n",
    "    try:\n",
    "        for new_tweets in pool.imap_unordered(query_tweets_once, queries):\n",
    "            all_tweets.extend(new_tweets)\n",
    "            logging.info(\"Got {} tweets ({} new).\".format(\n",
    "                len(all_tweets), len(new_tweets)))\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Program interrupted by user. Returning all tweets \"\n",
    "                     \"gathered so far.\")\n",
    "\n",
    "    return sorted(all_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
